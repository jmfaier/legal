{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bibliotecas\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import string\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe Preparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prep:\n",
    "    \n",
    "    def __init__(self,numero,processos,publicacoes):\n",
    "        self.numero = numero\n",
    "        self.processos = processos\n",
    "        self.publicacoes = publicacoes\n",
    "        self.big_table = big_table\n",
    "        #self.val = None\n",
    "        \n",
    "    def dq_msg(self, val):\n",
    "        if val != None:\n",
    "            return val\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    def text_process(self, mess):\n",
    "        \"\"\"\n",
    "        Takes in a string of text, then performs the following:\n",
    "        1. Remove all punctuation\n",
    "        2. Remove all stopwords\n",
    "        3. Returns a list of the cleaned text\n",
    "        \"\"\"\n",
    "        # Check characters to see if they are in punctuation\n",
    "        nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "        # Join the characters again to form the string.\n",
    "        nopunc = ''.join(nopunc)\n",
    "\n",
    "        # Now just remove any stopwords\n",
    "        return [word for word in nopunc.split() if word.lower() not in stopwords.words('portuguese')]\n",
    "\n",
    "    def text_process2(self,mess,opc):\n",
    "        \"\"\"\n",
    "        Takes in a string of text, then performs the following:\n",
    "        1. Remove all punctuation\n",
    "        2. Remove all stopwords\n",
    "        3. Returns a list of the cleaned text\n",
    "        \"\"\"\n",
    "        nowhitespace = [char for char in mess if char not in '\\t\\r\\x0b\\x0c']\n",
    "        nowhitespace = ''.join(nowhitespace)\n",
    "        nobarran = nowhitespace.split('\\n')\n",
    "        nobarran=' '.join(nobarran)\n",
    "        # Check characters to see if they are in punctuation\n",
    "        nopunc = [char for char in nobarran if char not in string.punctuation]\n",
    "        # Join the characters again to form the string.\n",
    "        nopunc = ''.join(nopunc)\n",
    "        # todas minusculas\n",
    "        nocaps = nopunc.lower()\n",
    "        if opc == 1:\n",
    "            return [word for word in nocaps.split() if word.lower() not in stopwords.words('portuguese')] #Remove stopwords e retorna a lista de termos\n",
    "        elif opc ==2:\n",
    "            return [word for word in nocaps.split()] #Retorna apenas a lista de termos\n",
    "        elif opc == 3:\n",
    "            return nocaps.lower() #Retorna uma string\n",
    "        \n",
    "    def prep_num_processo(self):\n",
    "        proc = self.numero #Fonte: andamentos\n",
    "        id_proc = np.array([self.numero]) #Fonte: cabeçalho\n",
    "        \n",
    "        #Numero do processo - Fonte: cabeçalho\n",
    "        df_id = pd.DataFrame(id_proc,columns=['id'])\n",
    "        \n",
    "        #Quebra do numero do processo - Fonte: andamentos\n",
    "        p = proc.split('.')\n",
    "\n",
    "        num = int(p[0].split('-')[0]+p[0].split('-')[1])\n",
    "        ano_inicio = int(p[1])\n",
    "        ramo = int(p[2])\n",
    "        tribunal = int(p[3])\n",
    "        vara_orig = int(p[4])\n",
    "\n",
    "        arr = np.array([[num, ano_inicio, ramo, tribunal, vara_orig]])\n",
    "\n",
    "        df_proc = pd.DataFrame(arr,columns=['num_seq','ano_inicio','ramo','tribunal','vara_orig'])\n",
    "        \n",
    "        #Concatenacao\n",
    "        df = pd.concat([df_id, df_proc],axis=1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prep_base(self):\n",
    "        classe = self.processos[0]['classe']\n",
    "        classe_area = self.processos[0]['classe_area']\n",
    "        assunto = self.processos[0]['assunto']\n",
    "        outros_assuntos = self.processos[0]['outros_assuntos']\n",
    "        juiz = self.processos[0]['juiz']\n",
    "        valor = self.processos[0]['valor']\n",
    "        audiencias = self.processos[0]['audiencias']\n",
    "        incidentes = self.processos[0]['incidentes']\n",
    "        total_andamentos = self.processos[0]['total_andamentos']\n",
    "\n",
    "        df_base = pd.DataFrame(np.array([[classe, classe_area, assunto, outros_assuntos, juiz, audiencias, incidentes,valor,total_andamentos]]), columns=['classe', 'classe_area', 'assunto', 'outros_assuntos', 'juiz', 'audiencias', 'incidentes','valor','total_andamentos'])\n",
    "        \n",
    "        return df_base\n",
    "    \n",
    "    def prep_partes(self):\n",
    "        a=np.array(self.processos[0]['partes'])\n",
    "        a.size\n",
    "        out_tipo = []\n",
    "        out_reqte= []\n",
    "        out_reqda = []\n",
    "        for i in range(a.size):\n",
    "            tipo = self.processos[0]['partes'][i]['tipo']\n",
    "            if tipo == 'Reqte' or tipo == 'Autor' or tipo == 'Exeqte':\n",
    "                out_reqte.append(self.processos[0]['partes'][i]['nome'])\n",
    "            elif tipo == 'Reqda' or tipo == 'Reqdo' or tipo == 'Ré' or tipo == 'Réu' or tipo == 'Exectdo' or tipo == 'TerIntCer':\n",
    "                out_reqda.append(self.processos[0]['partes'][i]['nome'])\n",
    "        arr=[[out_reqte, out_reqda]]\n",
    "        df_partes = pd.DataFrame(arr,columns = ['Reqte','Reqdo'])\n",
    "        \n",
    "        return df_partes\n",
    "    \n",
    "    def prep_advogados(self):\n",
    "        a=np.array(self.processos[0]['advogados'])\n",
    "        a.size\n",
    "        out_advogados = []\n",
    "        for i in range(a.size):\n",
    "            out_advogados.append(self.processos[0]['advogados'][i]['nome'])\n",
    "        arr=[[out_advogados]]\n",
    "        df_advogados = pd.DataFrame(arr,columns = ['Advogados'])\n",
    "        \n",
    "        return df_advogados\n",
    "    \n",
    "    def prep_andamentos(self):\n",
    "        a=np.array(self.processos[0]['andamentos'])\n",
    "        a.size\n",
    "        out_data = []\n",
    "        out_descricao = []\n",
    "        out_anexo = []\n",
    "        out_anexo_label = []\n",
    "        out_anexo_conteudo = []\n",
    "        for i in range(a.size):\n",
    "            out_data.append(self.processos[0]['andamentos'][i]['data'])\n",
    "            #out_descricao.append(self.processos[0]['andamentos'][i]['descricao'])\n",
    "            out_descricao.append(self.text_process2(self.processos[0]['andamentos'][i]['descricao'], opc=3))\n",
    "            if processos[0]['andamentos'][i]['anexo'] == 'Não existe anexo':\n",
    "                out_anexo_label.append(self.text_process2(self.processos[0]['andamentos'][0]['anexo'], opc=3))\n",
    "                out_anexo_conteudo.append(self.text_process2(self.processos[0]['andamentos'][0]['anexo'], opc=3))\n",
    "            else:\n",
    "                #out_anexo_label.append(self.processos[0]['andamentos'][i]['anexo']['label'])\n",
    "                #out_anexo_conteudo.append(self.processos[0]['andamentos'][i]['anexo']['conteudo']) \n",
    "                out_anexo_label.append(self.text_process2(self.processos[0]['andamentos'][i]['anexo']['label'], opc=3))\n",
    "                out_anexo_conteudo.append(self.text_process2(self.processos[0]['andamentos'][i]['anexo']['conteudo'],opc=3))\n",
    "\n",
    "        arr=[[out_data,out_descricao,out_anexo_label,out_anexo_conteudo]]\n",
    "        df_andamentos = pd.DataFrame(arr,columns = ['dt_andamentos','andamentos','anexo_label','anexo_conteudo'])\n",
    "        \n",
    "        return df_andamentos\n",
    "    \n",
    "    def prep_peticoes(self):\n",
    "        if processos[0]['peticoes'] is None:\n",
    "            data = []#{}\n",
    "            tipo = []#{}\n",
    "        else:\n",
    "            a=np.array(self.processos[0]['peticoes'])\n",
    "            a.size\n",
    "            data = []\n",
    "            tipo = []\n",
    "            for i in range(a.size):\n",
    "                data.append(self.processos[0]['peticoes'][i]['data'])\n",
    "                tipo.append(self.text_process2(self.processos[0]['peticoes'][i]['tipo'], opc=3))\n",
    "            \n",
    "        arr=[[data, tipo]]\n",
    "        df_peticoes = pd.DataFrame(arr,columns = ['dt_peticoes','peticoes_tipo'])\n",
    "        return df_peticoes\n",
    "    \n",
    "    def prep_publicacoes(self):\n",
    "        a=np.array(self.publicacoes)\n",
    "        total_publicacoes = a.size\n",
    "        \n",
    "        data = []\n",
    "        numeroInstancia = []\n",
    "        conteudo = []\n",
    "        numeroCNJ = []\n",
    "        numeroAntigo = []\n",
    "        numeroUnificado = []\n",
    "        numeroInstancia = []\n",
    "        anoPublicacao = []\n",
    "        dataPublicacao = []\n",
    "        diario = []\n",
    "        diarioUF = []\n",
    "        cidadeComarcaDescricao = []\n",
    "        varaDescricao  = []\n",
    "        arquivada = []\n",
    "        complemento = []\n",
    "        conteudo = []\n",
    "        despacho = []\n",
    "        #Empresa\n",
    "        codVinculo = []\n",
    "        vinculo = []\n",
    "        #Busca\n",
    "        codTermo = []\n",
    "        termoEncontrado = []\n",
    "        buscaLote = []\n",
    "        buscaLoteAno = []\n",
    "        buscaLoteMes = []\n",
    "        buscaLoteGrupo = []\n",
    "        #Correcoes\n",
    "        corrigido = []\n",
    "        corrigidoCidade = []\n",
    "        corrigidoConteudo = []\n",
    "        corrigidoDespacho = []\n",
    "        corrigidoNumero = []\n",
    "        corrigidoOrgao = []\n",
    "        corrigidoVara = []\n",
    "        conferido = []\n",
    "        for i in range(a.size):\n",
    "            data.append(self.publicacoes[i]['dataPublicacao'])\n",
    "            numeroInstancia.append(self.publicacoes[i]['numeroInstancia'])\n",
    "            conteudo.append(self.text_process2(self.publicacoes[i]['conteudo'], opc=3))\n",
    "            \n",
    "            numeroCNJ.append(self.publicacoes[i]['numeroCNJ'])\n",
    "            numeroAntigo.append(self.publicacoes[i]['numeroAntigo'])\n",
    "            numeroUnificado.append(self.publicacoes[i]['numeroUnificado'])\n",
    "            anoPublicacao.append(self.publicacoes[i]['anoPublicacao'])\n",
    "            dataPublicacao.append(self.publicacoes[i]['dataPublicacao'])\n",
    "            diario.append(self.publicacoes[i]['diario'])\n",
    "            diarioUF.append(self.publicacoes[i]['diarioUF'])\n",
    "            cidadeComarcaDescricao.append(self.publicacoes[i]['cidadeComarcaDescricao'])\n",
    "            varaDescricao.append(self.publicacoes[i]['varaDescricao'])\n",
    "            arquivada.append(self.publicacoes[i]['arquivada'])\n",
    "            complemento.append(self.publicacoes[i]['complemento'])\n",
    "            despacho.append(self.text_process2(self.publicacoes[i]['despacho'], opc=3))\n",
    "            #Empresa\n",
    "            codVinculo.append(self.publicacoes[i]['codVinculo'])\n",
    "            vinculo.append(self.publicacoes[i]['vinculo'])\n",
    "            #Busca\n",
    "            codTermo.append(self.publicacoes[i]['codTermo'])\n",
    "            termoEncontrado.append(self.publicacoes[i]['termoEncontrado'])\n",
    "            buscaLote.append(self.publicacoes[i]['buscaLote'])\n",
    "            buscaLoteAno.append(self.publicacoes[i]['buscaLoteAno'])\n",
    "            buscaLoteMes.append(self.publicacoes[i]['buscaLoteMes'])\n",
    "            buscaLoteGrupo.append(self.publicacoes[i]['buscaLoteGrupo'])\n",
    "            #Correcoes\n",
    "            corrigido.append(self.publicacoes[i]['corrigido'])\n",
    "            corrigidoCidade.append(self.publicacoes[i]['corrigidoCidade'])\n",
    "            corrigidoConteudo.append(self.publicacoes[i]['corrigidoConteudo'])\n",
    "            corrigidoDespacho.append(self.publicacoes[i]['corrigidoDespacho'])\n",
    "            corrigidoNumero.append(self.publicacoes[i]['corrigidoNumero'])\n",
    "            corrigidoOrgao.append(self.publicacoes[i]['corrigidoOrgao'])\n",
    "            corrigidoVara.append(self.publicacoes[i]['corrigidoVara'])\n",
    "            conferido.append(self.publicacoes[i]['conferido'])\n",
    "\n",
    "        arr=[[total_publicacoes,data,numeroInstancia,conteudo,numeroCNJ,numeroAntigo,numeroUnificado,anoPublicacao,dataPublicacao,diario,diarioUF,cidadeComarcaDescricao,varaDescricao,arquivada,complemento,despacho,codVinculo,vinculo,codTermo,termoEncontrado,buscaLote,buscaLoteAno,buscaLoteMes,buscaLoteGrupo,corrigido,corrigidoCidade,corrigidoConteudo,corrigidoDespacho,corrigidoNumero,corrigidoOrgao,corrigidoVara,conferido]]\n",
    "        df_publicacoes = pd.DataFrame(arr,columns = ['total_publicacoes','data','numeroInstancia','conteudo','numeroCNJ','numeroAntigo','numeroUnificado','anoPublicacao','dataPublicacao','diario','diarioUF','cidadeComarcaDescricao','varaDescricao','arquivada','complemento','despacho','codVinculo','vinculo','codTermo','termoEncontrado','buscaLote','buscaLoteAno','buscaLoteMes','buscaLoteGrupo','corrigido','corrigidoCidade','corrigidoConteudo','corrigidoDespacho','corrigidoNumero','corrigidoOrgao','corrigidoVara','conferido'])\n",
    "        \n",
    "        return df_publicacoes\n",
    "\n",
    "    def prep_publicacoes2(self):\n",
    "        a=np.array(self.publicacoes)\n",
    "        total_publicacoes = a.size\n",
    "        \n",
    "        data = []\n",
    "        conteudo = []\n",
    "        despacho = []\n",
    "        anoPublicacao = []\n",
    "        caderno = []\n",
    "        cadernoDescricao = []\n",
    "        cadernoParte = []\n",
    "        cidadeComarcaDescricao = []\n",
    "        dataDivulgacao = []\n",
    "        dataPublicacao = []\n",
    "        diario = []\n",
    "        diarioEdicao = []\n",
    "        diarioUF = []\n",
    "        numeroAntigo = []\n",
    "        numeroCNJ = []\n",
    "        numeroInstancia = []\n",
    "        numeroUnificado = []\n",
    "        termoEncontrado = []\n",
    "        varaDescricao = []\n",
    "        vinculo = []\n",
    "        \n",
    "        #data = []\n",
    "        ##numeroInstancia = []\n",
    "        #conteudo = []\n",
    "        ##numeroCNJ = []\n",
    "        ##numeroAntigo = []\n",
    "        ##numeroUnificado = []\n",
    "        ##anoPublicacao = []\n",
    "        ##dataPublicacao = []\n",
    "        ##diario = []\n",
    "        ##diarioUF = []\n",
    "        ##cidadeComarcaDescricao = []\n",
    "        ##varaDescricao  = []\n",
    "        #arquivada = []\n",
    "        #complemento = []\n",
    "        #conteudo = []\n",
    "        ##despacho = []\n",
    "        #Empresa\n",
    "        #codVinculo = []\n",
    "        ##vinculo = []\n",
    "        #Busca\n",
    "        #codTermo = []\n",
    "        ##termoEncontrado = []\n",
    "        #buscaLote = []\n",
    "        #buscaLoteAno = []\n",
    "        #buscaLoteMes = []\n",
    "        #buscaLoteGrupo = []\n",
    "        #Correcoes\n",
    "        #corrigido = []\n",
    "        #corrigidoCidade = []\n",
    "        #corrigidoConteudo = []\n",
    "        #corrigidoDespacho = []\n",
    "        #corrigidoNumero = []\n",
    "        #corrigidoOrgao = []\n",
    "        #corrigidoVara = []\n",
    "        #conferido = []\n",
    "        for i in range(a.size):\n",
    "            data.append(self.publicacoes[i]['dataPublicacao'])\n",
    "            numeroInstancia.append(self.publicacoes[i]['numeroInstancia'])\n",
    "            conteudo.append(self.publicacoes[i]['Conteudo'])\n",
    "            \n",
    "            numeroCNJ.append(self.publicacoes[i]['numeroCNJ'])\n",
    "            numeroAntigo.append(self.publicacoes[i]['numeroAntigo'])\n",
    "            numeroUnificado.append(self.publicacoes[i]['numeroUnificado'])\n",
    "            anoPublicacao.append(self.publicacoes[i]['anoPublicacao'])\n",
    "            dataPublicacao.append(self.publicacoes[i]['dataPublicacao'])\n",
    "            diario.append(self.publicacoes[i]['diario'])\n",
    "            diarioUF.append(self.publicacoes[i]['diarioUF'])\n",
    "            cidadeComarcaDescricao.append(self.publicacoes[i]['cidadeComarcaDescricao'])\n",
    "            varaDescricao.append(self.publicacoes[i]['varaDescricao'])\n",
    "            \n",
    "            #arquivada.append(self.publicacoes[i]['arquivada'])\n",
    "            #complemento.append(self.publicacoes[i]['complemento'])\n",
    "            despacho.append(self.publicacoes[i]['Despacho'])\n",
    "            #Empresa\n",
    "            #codVinculo.append(self.publicacoes[i]['codVinculo'])\n",
    "            vinculo.append(self.publicacoes[i]['vinculo'])\n",
    "            #Busca\n",
    "            termoEncontrado.append(self.publicacoes[i]['termoEncontrado'])\n",
    "\n",
    "        arr=[[data,numeroInstancia,conteudo,numeroCNJ,numeroAntigo,numeroUnificado,anoPublicacao,dataPublicacao,diario,diarioUF,cidadeComarcaDescricao,varaDescricao,despacho,vinculo,termoEncontrado]]\n",
    "        df_publicacoes = pd.DataFrame(arr,columns = ['data','numeroInstancia','conteudo','numeroCNJ','numeroAntigo','numeroUnificado','anoPublicacao','dataPublicacao','diario','diarioUF','cidadeComarcaDescricao','varaDescricao','despacho','vinculo','termoEncontrado'])\n",
    "        \n",
    "        return df_publicacoes\n",
    "        #return arr\n",
    "    \n",
    "    \n",
    "    def qd_text(self, tag, mess, padrao):\n",
    "        if padrao != '': #Caso em que a mensagem e tag são separados por 3 espaços\n",
    "            quebra = mess.split(padrao)\n",
    "            mess = quebra[1:]\n",
    "            tag = quebra[0]\n",
    "        elif padrao == '': #Caso em que há um atributo para tag e outro para a mensagem         \n",
    "            if type(mess) == dict:\n",
    "                tag = mess['label']\n",
    "                mess = mess['conteudo']\n",
    "\n",
    "        return tag, mess\n",
    "\n",
    "\n",
    "    def prep_tags(self, atr):\n",
    "        num = []\n",
    "        n = []\n",
    "        andamentos = []\n",
    "        label = []\n",
    "        dt = []\n",
    "        for i in range(len(self.big_table[atr['texto']])):\n",
    "            a = self.big_table[atr['texto']][i]\n",
    "            l = self.big_table[atr['label']][i]\n",
    "            d = self.big_table[atr['data']][i]\n",
    "            for j in range(len(a)):\n",
    "                n = n + [self.big_table[atr['id']][i]]\n",
    "\n",
    "            num = num + n\n",
    "            n = []\n",
    "            andamentos = andamentos + a\n",
    "            label = label + l\n",
    "            dt = dt + d\n",
    "        #print(andamentos)\n",
    "        arr_andamentos = np.array(andamentos)\n",
    "        arr_label = np.array(label)\n",
    "\n",
    "        messages = ['']\n",
    "        tags = []\n",
    "        for i in range(arr_andamentos.size):\n",
    "            tag = arr_label[i]\n",
    "            mess = arr_andamentos[i]\n",
    "\n",
    "            tag, mess = self.qd_text(tag, mess, atr['padrao'])\n",
    "\n",
    "            messages.append(mess) \n",
    "            tags.append(tag)\n",
    "\n",
    "        return num, dt, tags, messages\n",
    "\n",
    "    def qd_data(self,data):\n",
    "        dt = datetime.strptime(data,'%b %d %Y %H:%M:%S:%f%p')\n",
    "        return str(dt.day) +'/'+ str(dt.month) +'/'+ str(dt.year)\n",
    "\n",
    "    def prep_alvo(self,item,alvo):\n",
    "        a = 0\n",
    "        n = len(np.array(alvo))\n",
    "        for i in range(n):\n",
    "            if item == alvo[i]:\n",
    "                a = 1\n",
    "        return a\n",
    "    \n",
    "    def prep_alvo2(self,item,alvo):\n",
    "        #i = 0\n",
    "        item = str(item)\n",
    "        l = int(len(alvo)/2)\n",
    "        for j in range(l):\n",
    "            a = alvo['alvo'+str(j)]\n",
    "            t = alvo['termos'+str(j)]\n",
    "            flag = None\n",
    "            for k in range(len(t)):\n",
    "                #print(item)\n",
    "                n = item.find(t[k])\n",
    "                if n >=0:\n",
    "                    flag = a\n",
    "        return flag\n",
    "        #print(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objeto Proc Itau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"data/itau\"\n",
    "filelist = os.listdir(path_data)\n",
    "big_table = pd.DataFrame()\n",
    "n = len(filelist)\n",
    "#n = 20\n",
    "for i in range(n):\n",
    "    arquivo_json = open(path_data+'/'+filelist[i],'r')\n",
    "    dados_json = json.load(arquivo_json)\n",
    "    \n",
    "    #Grandes categorias\n",
    "    numero = dados_json['numero']\n",
    "    processos = dados_json['processos']\n",
    "    publicacoes = dados_json['publicacoes']\n",
    "    \n",
    "    proc = Prep(numero, processos, publicacoes)\n",
    "    df_numero = proc.prep_num_processo()\n",
    "    df_base = proc.prep_base()\n",
    "    df_partes = proc.prep_partes()\n",
    "    df_advogados = proc.prep_advogados()\n",
    "    df_andamentos = proc.prep_andamentos()\n",
    "    df_peticoes = proc.prep_peticoes()\n",
    "    df_publicacoes = proc.prep_publicacoes()\n",
    "    df = pd.concat([df_numero,df_base,df_partes,df_advogados,df_andamentos,df_peticoes,df_publicacoes],axis=1)\n",
    "    big_table = pd.concat([big_table,df],axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = ['id', 'num_seq', 'ano_inicio', 'ramo', 'tribunal', 'vara_orig',\n",
    "       'classe', 'classe_area', 'assunto', 'outros_assuntos', 'juiz',\n",
    "       'audiencias', 'incidentes', 'valor', 'total_andamentos', 'Reqte',\n",
    "       'Reqdo', 'Advogados']\n",
    "\n",
    "proc.big_table = big_table\n",
    "\n",
    "#Armazenagem da Bt\n",
    "#big_table[colunas].to_csv('data/output/big_table.csv', sep='\\t', index= False, encoding='utf-16') #ou latin-1 iso-8859-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num, dt, tags, messages = proc.prep_tags(atributos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TBs, BOWs e TFIDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#Setup dos alvos\n",
    "atr_alvo = [['julgada procedente a ação'],\n",
    "            ['remetido ao dje','conclusos para decisão'],\n",
    "            ['remetido ao dje','conclusos para decisão'],\n",
    "            ['remetido ao dje','conclusos para decisão'],\n",
    "            ['remetido ao dje','conclusos para decisão']]\n",
    "\n",
    "atr_alvo2 =  [{'alvo0':1,'termos0': ['julgada procedente a ação','julgada procedente'],\n",
    "      'alvo1':0,'termos1': ['julgada improcedente a ação','julgada improcedente'],\n",
    "      'alvo2':-1,'termos2': ['julgada procedente em parte a ação','julgada procedente em parte']},\n",
    "        {'alvo0':1,'termos0': ['julgada procedente a ação'],\n",
    "      'alvo1':0,'termos1': ['julgada improcedente a ação'],\n",
    "      'alvo2':-1,'termos2': ['julgada procedente em parte a ação']},\n",
    "        {'alvo0':1,'termos0': ['julgada procedente a ação'],\n",
    "      'alvo1':0,'termos1': ['julgada improcedente a ação'],\n",
    "      'alvo2':-1,'termos2': ['julgada procedente em parte a ação']},\n",
    "        {'alvo0':1,'termos0': ['julgada procedente a ação'],\n",
    "      'alvo1':0,'termos1': ['julgada improcedente a ação'],\n",
    "      'alvo2':-1,'termos2': ['julgada procedente em parte a ação']},\n",
    "        {'alvo0':1,'termos0': ['julgada procedente a ação'],\n",
    "      'alvo1':0,'termos1': ['julgada improcedente a ação'],\n",
    "      'alvo2':-1,'termos2': ['julgada procedente em parte a ação']}]\n",
    "\n",
    "#Setup do atributo de texto a ser analisado\n",
    "atr = [{'id':'id', 'data':'dt_andamentos','label':'andamentos', 'texto':'andamentos','padrao':'   '},#Andamentos [0]\n",
    "       {'id':'id', 'data':'dt_andamentos','label':'anexo_label', 'texto':'anexo_conteudo','padrao':''},#Anexos [1]\n",
    "       {'id':'id', 'data':'dt_peticoes','label':'peticoes_tipo', 'texto':'peticoes_tipo','padrao':''},#Peticoes [2]\n",
    "       {'id':'id', 'data':'data','label':'conteudo', 'texto':'conteudo','padrao':''},#Publicacoes [3]\n",
    "       {'id':'id', 'data':'data','label':'conteudo', 'texto':'despacho','padrao':''}]#Despachos [4]\n",
    "\n",
    "tb = ['andamentos', 'anexos', 'peticoes', 'publicacoes', 'despachos']\n",
    "#Criação e armazenagem das TBs\n",
    "#tb = ['anexos']\n",
    "\n",
    "bte = pd.DataFrame()\n",
    "bt_id = pd.DataFrame()\n",
    "\n",
    "for i in range(len(tb)):\n",
    "    alvo = atr_alvo[i]\n",
    "    alvo2 = atr_alvo2[i]\n",
    "    atributos = atr[i]\n",
    "    num, dt, tags, messages = proc.prep_tags(atributos)\n",
    "\n",
    "    df_num = pd.DataFrame(num,columns = ['num'])\n",
    "    df_dt = pd.DataFrame(dt,columns = ['dt'])\n",
    "    if atr[i]['data'] == 'data': df_dt['dt'] = df_dt['dt'].apply(proc.qd_data)\n",
    "    df_tags = pd.DataFrame(tags,columns = ['tags'])\n",
    "    df_messages = pd.DataFrame(messages, columns=['messages'])\n",
    "      \n",
    "    bt = pd.concat([df_num, df_dt, df_tags,df_messages],axis=1)\n",
    "    \n",
    "    #Enriquecimento\n",
    "    bt['messages'] = bt['messages'].apply(proc.dq_msg) \n",
    "    bt['length_messages'] = bt['messages'].apply(len)\n",
    "    \n",
    "    #Alvos\n",
    "    bt['alvo'] = bt['tags'].apply(proc.prep_alvo, args=[alvo])\n",
    "    bbb = bt['tags'].apply(proc.prep_alvo2, args=[alvo2])\n",
    "    \n",
    "    #BOW   \n",
    "    #bow = CountVectorizer(analyzer=proc.text_process).fit(bt['messages'])\n",
    "    #df_bow = bow.transform(bt['messages'])\n",
    "    #df = pd.DataFrame(np.array(df_bow.todense()),columns= bow.get_feature_names())\n",
    "    #tb_bow = pd.concat([df_num, df_dt, df],axis=1)\n",
    "    \n",
    "    #TFIDF\n",
    "    #tfidf = TfidfTransformer().fit(df_bow)\n",
    "    #df_tfidf = tfidf.transform(df_bow)\n",
    "    #df = pd.DataFrame(np.array(df_tfidf.todense()),columns= bow.get_feature_names())\n",
    "    #tb_tfidf = pd.concat([df_num, df_dt, df],axis=1)\n",
    "    \n",
    "    #Armazena TBs\n",
    "    #file = 'data/output/tb_' + tb[i] + '.csv'\n",
    "    #bt.to_csv(file, sep='\\t', index= False, encoding='utf-16') #ou latin-1 iso-8859-1\n",
    "    \n",
    "    #Armazena BOWs\n",
    "    #file = 'data/output/bow_' + tb[i] + '.csv'\n",
    "    #tb_bow.to_csv(file, sep='\\t', index= False, encoding='utf-16') #ou latin-1 iso-8859-1\n",
    "    \n",
    "    #Armazena TFIDFs\n",
    "    #file = 'data/output/tfidf_' + tb[i] + '.csv'\n",
    "    #tb_tfidf.to_csv(file, sep='\\t', index= False, encoding='utf-16') #ou latin-1 iso-8859-1\n",
    "    \n",
    "    \n",
    "    #Soma-se os alvos\n",
    "    a = bt.groupby(['num']).sum() #Agrupa pelo numero, somando-se os valoree\n",
    "    a = a.add_suffix('_sum_'+tb[i]).reset_index() # Adiciona um sufixo as colunas de count recem criadas\n",
    "    bt_id['id'] = big_table['id']\n",
    "    btt = pd.merge(bt_id, a, how='left', left_on='id', right_on='num')\n",
    "\n",
    "    bte[i] = [btt]\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvamento da Big Table\n",
    "bt = big_table.merge(bte[0][0],on='id').merge(bte[1][0], on='id').merge(bte[2][0], on='id').merge(bte[3][0], on='id').merge(bte[4][0], on='id')\n",
    "#Armazenagem da Big Table\n",
    "bt.to_csv('data/output/big_table.csv', sep='\\t', index= False, encoding='utf-16') #ou latin-1 iso-8859-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dicionario de termos\n",
    "sw = stopwords.words('portuguese')\n",
    "df_sw = pd.DataFrame(sw, columns=['stopwords'])\n",
    "file = 'data/output/stopwords.csv'\n",
    "df_sw.to_csv(file, sep='\\t', index= False, encoding='utf-16') #ou latin-1 iso-8859-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objeto Proc CPFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"data/cpfl/publicacoes\"\n",
    "filelist = os.listdir(path_data)\n",
    "big_table = pd.DataFrame()\n",
    "n = len(filelist)\n",
    "processos = []\n",
    "for i in range(n):\n",
    "    arquivo_json = open(path_data+'/'+filelist[i], mode='r', encoding='utf-8')\n",
    "    dados_json = json.load(arquivo_json)\n",
    "    #df_json = pd.DataFrame(dados_json)\n",
    "\n",
    "    publicacoes = dados_json\n",
    "    proc = Prep([], [], publicacoes)\n",
    "    df_publicacoes = proc.prep_publicacoes2()\n",
    "\n",
    "    a=np.array(publicacoes)\n",
    "    df= pd.DataFrame(a)\n",
    "    for j in range(a.size):\n",
    "        #processos.append(df[0][j]['numeroCNJ'])\n",
    "        processos.append(df[0][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analise CPFL\n",
    "df= pd.DataFrame(processos)\n",
    "df.groupby(['anoPublicacao']).describe()\n",
    "df['numeroCNJ'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ramos\n",
    "#civel = 8\n",
    "#trabalhista = 5\n",
    "#federais = 4\n",
    "#eleitorais = 6\n",
    "#Tribunal ou UF\n",
    "#if trabalhista:\n",
    "#    tribunal #Ex.: TRT15 = 15\n",
    "#else:\n",
    "#    UF #Ex.: SP = 26\n",
    "#big_table['vara_orig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = np.zeros(len(bt['tags']),dtype=int)\n",
    "#n = len(np.array(alvo))\n",
    "\n",
    "#b = bt['tags'].apply(prep_alvo, args=[alvo])\n",
    "\n",
    "#for i in range(n):\n",
    "    #b = bt['tags'].apply(prep_alvo, args=[alvo])\n",
    "    #a = a + b\n",
    "#bt['alvo'] = a\n",
    "#bt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow = CountVectorizer(analyzer=proc.text_process).fit(df_descritivo['messages'])\n",
    "#bow.to_dense()\n",
    "#type(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf = TfidfTransformer().fit(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe Analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analise:\n",
    "    \n",
    "    def __init__(self,proc):\n",
    "        self.proc = proc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'num_seq', 'ano_inicio', 'ramo', 'tribunal', 'vara_orig',\n",
       "       'classe', 'classe_area', 'assunto', 'outros_assuntos', 'juiz',\n",
       "       'audiencias', 'incidentes', 'valor', 'total_andamentos', 'Reqte',\n",
       "       'Reqdo', 'Advogados', 'dt_andamentos', 'andamentos', 'anexo_label',\n",
       "       'anexo_conteudo', 'dt_peticoes', 'peticoes_tipo', 'total_publicacoes',\n",
       "       'data', 'numeroInstancia', 'conteudo', 'numeroCNJ', 'numeroAntigo',\n",
       "       'numeroUnificado', 'anoPublicacao', 'dataPublicacao', 'diario',\n",
       "       'diarioUF', 'cidadeComarcaDescricao', 'varaDescricao', 'arquivada',\n",
       "       'complemento', 'despacho', 'codVinculo', 'vinculo', 'codTermo',\n",
       "       'termoEncontrado', 'buscaLote', 'buscaLoteAno', 'buscaLoteMes',\n",
       "       'buscaLoteGrupo', 'corrigido', 'corrigidoCidade', 'corrigidoConteudo',\n",
       "       'corrigidoDespacho', 'corrigidoNumero', 'corrigidoOrgao',\n",
       "       'corrigidoVara', 'conferido'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count_nonzero</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ramo</th>\n",
       "      <th>classe_area</th>\n",
       "      <th>tribunal</th>\n",
       "      <th>ano_inicio</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"23\" valign=\"top\">8</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">Criminal</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">26</th>\n",
       "      <th>2010</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">Cível</th>\n",
       "      <th rowspan=\"15\" valign=\"top\">26</th>\n",
       "      <th>1999</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     count_nonzero\n",
       "                                                id\n",
       "ramo classe_area tribunal ano_inicio              \n",
       "8    Criminal    26       2010                   2\n",
       "                          2011                   2\n",
       "                          2012                   2\n",
       "                          2013                   1\n",
       "                          2014                   2\n",
       "                          2015                   3\n",
       "                          2016                   2\n",
       "                          2017                   1\n",
       "     Cível       26       1999                   2\n",
       "                          2002                   1\n",
       "                          2003                   3\n",
       "                          2006                   1\n",
       "                          2007                  13\n",
       "                          2008                  13\n",
       "                          2009                  15\n",
       "                          2010                   7\n",
       "                          2011                  19\n",
       "                          2012                  17\n",
       "                          2013                  20\n",
       "                          2014                  22\n",
       "                          2015                  24\n",
       "                          2016                  33\n",
       "                          2017                 259"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pivot Table da parte estruturada\n",
    "pt = pd.pivot_table(big_table, values=['id'], index=['ramo','classe_area','tribunal','ano_inicio'],aggfunc= [np.count_nonzero])\n",
    "pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exploração das Tags e Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos = atr[1] #Selecione o atributo a ser analisado\n",
    "num, dt, tags, messages = proc.prep_tags(atributos)\n",
    "\n",
    "df_tags = pd.DataFrame(tags,columns = ['tags'])\n",
    "df_messages = pd.DataFrame(messages, columns=['messages'])\n",
    "bt = pd.concat([df_tags,df_messages],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_descritivo = bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [existe, anexo]\n",
       "1    [existe, anexo]\n",
       "2      [Auto, senha]\n",
       "3    [existe, anexo]\n",
       "4    [existe, anexo]\n",
       "Name: messages, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bb6c88d8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Impressão das mensagens e enumeração\n",
    "#for message_no, message in enumerate(messages[:10]):\n",
    "#    print(message_no, message)\n",
    "#    print('\\n')\n",
    "#Algumas estatísticas descritivas básicas das tags e messages\n",
    "#df_descritivo.describe()\n",
    "#Descrição das mensagens, agrupadas por tags\n",
    "df_descritivo.groupby('tags').describe()\n",
    "#Qualificação das mensagens de andamentos\n",
    "df_descritivo['messages'] = df_descritivo['messages'].apply(proc.dq_msg)\n",
    "df_descritivo['length'] = df_descritivo['messages'].apply(len)\n",
    "#Distribuição do comprimento das mensagens de andamentos\n",
    "df_descritivo['length'].plot(kind='hist', bins=50)\n",
    "#Estatísticas descritivas do comprimentos das mensagens de andamentos\n",
    "df_descritivo['length'].describe() \n",
    "#Análise da mensagem com maior numero de termos\n",
    "df_descritivo.sort_values(['length'],ascending=False)[1:100]\n",
    "#Analise de alguns comprimentos de mensagens relevantes\n",
    "#length_msg = [15115, 14912, 12222, 12086, 11617, 11458, 11408, 11270, 11089, 11051] \n",
    "#for i in range(len(length_msg)):\n",
    "#    print(df_descritivo[df_descritivo['length'] == length_msg[i]]['tags'].iloc[0])\n",
    "#    #print(df_descritivo[df_descritivo['length'] == length_msg[i]]['messages'].iloc[0])\n",
    "#Analise da distribuição do comprimento das mensagens de uma tag específica\n",
    "#tag_especifica = ['Remetido ao DJE', 'Decisão Proferida','Julgada Improcedente a Ação - Sentença Completa']\n",
    "#for i in range(len(tag_especifica)):\n",
    "#    df_descritivo[df_descritivo['tags'] == tag_especifica[i]].hist(column='length', by='tags', bins=50,figsize=(12,4))\n",
    "#Visualização das stopwords em portugues\n",
    "df_sw = pd.DataFrame(stopwords.words('portuguese'),columns=['stopwords']) # Cuidado o NÃO é uma stopword\n",
    "df_sw.head()\n",
    "#Visualizacao das tags e mensagens originais\n",
    "df_descritivo.head()\n",
    "#Visualizacao das mensagens \"tokenized\"\n",
    "df_descritivo['messages'].head(5).apply(proc.text_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploração da BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(analyzer=proc.text_process).fit(df_descritivo['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantidade de termos no dicionario\n",
    "print(len(bow.vocabulary_)) #Numero de termos no vocabulario\n",
    "#Termos do dicionario\n",
    "bow.vocabulary_\n",
    "#Selecao de uma message específica\n",
    "m1 = df_descritivo['messages'][0]\n",
    "#Veja a BOW da message\n",
    "bow1 = bow.transform([m1])\n",
    "print(bow1)\n",
    "#Selecione o termo de maior frequencia\n",
    "#print(bow.get_feature_names()[35934])\n",
    "#Analise de termos relevantes\n",
    "termos = bow.get_feature_names()\n",
    "df_termos = pd.DataFrame(termos,columns=['termos'])\n",
    "df_termos[df_termos['termos'].str.contains('valor')]\n",
    "#Transformacao da BOW em um DataFrame esparso completo de mensagens\n",
    "df_bow = bow.transform(df_descritivo['messages'])\n",
    "#Algumas informações técnicas da BOW\n",
    "print('Shape of Sparse Matrix: ', df_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', df_bow.nnz)\n",
    "sparsity = (100.0 * df_bow.nnz / (df_bow.shape[0] * df_bow.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peso e Normalização com TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploração da TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer().fit(df_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 473)\t0.707106781187\n",
      "  (0, 356)\t0.707106781187\n",
      "624\n",
      "  (0, 473)\t0.707106781187\n",
      "  (0, 356)\t0.707106781187\n",
      "  (1, 473)\t0.707106781187\n",
      "  (1, 356)\t0.707106781187\n",
      "  (2, 633)\t0.707106781187\n",
      "  (2, 116)\t0.707106781187\n",
      "  (3, 473)\t0.707106781187\n",
      "  (3, 356)\t0.707106781187\n",
      "  (4, 473)\t0.707106781187\n",
      "  (4, 356)\t0.707106781187\n",
      "  (5, 473)\t0.707106781187\n",
      "  (5, 356)\t0.707106781187\n",
      "  (6, 473)\t0.707106781187\n",
      "  (6, 356)\t0.707106781187\n",
      "  (7, 473)\t0.707106781187\n",
      "  (7, 356)\t0.707106781187\n",
      "  (8, 669)\t0.0572384267467\n",
      "  (8, 658)\t0.0760925437208\n",
      "  (8, 652)\t0.0819763170562\n",
      "  (8, 647)\t0.0760925437208\n",
      "  (8, 646)\t0.0660341667681\n",
      "  (8, 640)\t0.0572384267467\n",
      "  (8, 623)\t0.0686798659735\n",
      "  (8, 607)\t0.0660341667681\n",
      "  (8, 602)\t0.0819763170562\n",
      "  :\t:\n",
      "  (195, 116)\t0.707106781187\n",
      "  (196, 633)\t0.707106781187\n",
      "  (196, 116)\t0.707106781187\n",
      "  (197, 633)\t0.707106781187\n",
      "  (197, 116)\t0.707106781187\n",
      "  (198, 633)\t0.707106781187\n",
      "  (198, 116)\t0.707106781187\n",
      "  (199, 633)\t0.707106781187\n",
      "  (199, 116)\t0.707106781187\n",
      "  (200, 633)\t0.707106781187\n",
      "  (200, 116)\t0.707106781187\n",
      "  (201, 633)\t0.707106781187\n",
      "  (201, 116)\t0.707106781187\n",
      "  (202, 633)\t0.707106781187\n",
      "  (202, 116)\t0.707106781187\n",
      "  (203, 633)\t0.707106781187\n",
      "  (203, 116)\t0.707106781187\n",
      "  (204, 633)\t0.707106781187\n",
      "  (204, 116)\t0.707106781187\n",
      "  (205, 633)\t0.707106781187\n",
      "  (205, 116)\t0.707106781187\n",
      "  (206, 633)\t0.707106781187\n",
      "  (206, 116)\t0.707106781187\n",
      "  (207, 633)\t0.707106781187\n",
      "  (207, 116)\t0.707106781187\n"
     ]
    }
   ],
   "source": [
    "#Pesos dos termos em uma mensagem, usando TF-IDF \n",
    "tfidf1 = tfidf.transform(bow1)\n",
    "print(tfidf1)\n",
    "#Verificar apenas a IDF de termos relevantes\n",
    "#tfidf.idf_[bow.vocabulary_['valor']]\n",
    "#Transforma as mensagens do TD_BOW em mensagens do DF_TFIDF\n",
    "df_tfidf = tfidf.transform(df_bow)\n",
    "#Analise da TF-IDF\n",
    "print(df_descritivo.size)\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
