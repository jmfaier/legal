{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from xml.dom import minidom\n",
    "#arquivo = open('data/cpfl/2018/Publicações  PARCERIA TERADATA AnoPublicação 2018 arquivo 1-3.xml','r')\n",
    "#xmldoc = minidom.parse(arquivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe Preparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prep:\n",
    "    \n",
    "    def __init__(self,numero,processos,publicacoes):\n",
    "        self.numero = numero\n",
    "        self.processos = processos\n",
    "        self.publicacoes = publicacoes\n",
    "        self.big_table = big_table\n",
    "        #self.val = None\n",
    "        \n",
    "    def prep_num_processo(self):\n",
    "        proc = self.numero #Fonte: andamentos\n",
    "        id_proc = np.array([self.numero]) #Fonte: cabeçalho\n",
    "        \n",
    "        #Numero do processo - Fonte: cabeçalho\n",
    "        df_id = pd.DataFrame(id_proc,columns=['id'])\n",
    "        \n",
    "        #Quebra do numero do processo - Fonte: andamentos\n",
    "        p = proc.split('.')\n",
    "\n",
    "        num = int(p[0].split('-')[0]+p[0].split('-')[1])\n",
    "        ano_inicio = int(p[1])\n",
    "        ramo = int(p[2])\n",
    "        tribunal = int(p[3])\n",
    "        vara_orig = int(p[4])\n",
    "\n",
    "        arr = np.array([[num, ano_inicio, ramo, tribunal, vara_orig]])\n",
    "\n",
    "        df_proc = pd.DataFrame(arr,columns=['num_seq','ano_inicio','ramo','tribunal','vara_orig'])\n",
    "        \n",
    "        #Concatenacao\n",
    "        df = pd.concat([df_id, df_proc],axis=1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prep_base(self):\n",
    "        classe = self.processos[0]['classe']\n",
    "        classe_area = self.processos[0]['classe_area']\n",
    "        assunto = self.processos[0]['assunto']\n",
    "        outros_assuntos = self.processos[0]['outros_assuntos']\n",
    "        juiz = self.processos[0]['juiz']\n",
    "        valor = self.processos[0]['valor']\n",
    "        audiencias = self.processos[0]['audiencias']\n",
    "        incidentes = self.processos[0]['audiencias']\n",
    "        total_andamentos = self.processos[0]['total_andamentos']\n",
    "\n",
    "        df_base = pd.DataFrame(np.array([[classe, classe_area, assunto, outros_assuntos, juiz, audiencias, incidentes,valor,total_andamentos]]), columns=['classe', 'classe_area', 'assunto', 'outros_assuntos', 'juiz', 'audiencias', 'incidentes','valor','total_andamentos'])\n",
    "        \n",
    "        return df_base\n",
    "    \n",
    "    def prep_partes(self):\n",
    "        a=np.array(self.processos[0]['partes'])\n",
    "        a.size\n",
    "        out_tipo = []\n",
    "        out_reqte= []\n",
    "        out_reqda = []\n",
    "        for i in range(a.size):\n",
    "            tipo = self.processos[0]['partes'][i]['tipo']\n",
    "            if tipo == 'Reqte' or tipo == 'Autor' or tipo == 'Exeqte':\n",
    "                out_reqte.append(self.processos[0]['partes'][i]['nome'])\n",
    "            elif tipo == 'Reqda' or tipo == 'Reqdo' or tipo == 'Ré' or tipo == 'Réu' or tipo == 'Exectdo' or tipo == 'TerIntCer':\n",
    "                out_reqda.append(self.processos[0]['partes'][i]['nome'])\n",
    "        arr=[[out_reqte, out_reqda]]\n",
    "        df_partes = pd.DataFrame(arr,columns = ['Reqte','Reqdo'])\n",
    "        \n",
    "        return df_partes\n",
    "    \n",
    "    def prep_advogados(self):\n",
    "        a=np.array(self.processos[0]['advogados'])\n",
    "        a.size\n",
    "        out_advogados = []\n",
    "        for i in range(a.size):\n",
    "            out_advogados.append(self.processos[0]['advogados'][i]['nome'])\n",
    "        arr=[[out_advogados]]\n",
    "        df_advogados = pd.DataFrame(arr,columns = ['Advogados'])\n",
    "        \n",
    "        return df_advogados\n",
    "    \n",
    "    def prep_andamentos(self):\n",
    "        a=np.array(self.processos[0]['andamentos'])\n",
    "        a.size\n",
    "        out_data = []\n",
    "        out_descricao = []\n",
    "        out_anexo = []\n",
    "        out_anexo_label = []\n",
    "        out_anexo_conteudo = []\n",
    "        for i in range(a.size):\n",
    "            out_data.append(self.processos[0]['andamentos'][i]['data'])\n",
    "            out_descricao.append(self.processos[0]['andamentos'][i]['descricao'])\n",
    "            if processos[0]['andamentos'][i]['anexo'] == 'Não existe anexo':\n",
    "                out_anexo_label.append(self.processos[0]['andamentos'][0]['anexo'])\n",
    "                out_anexo_conteudo.append(self.processos[0]['andamentos'][0]['anexo'])\n",
    "            else:\n",
    "                out_anexo_label.append(self.processos[0]['andamentos'][i]['anexo']['label'])\n",
    "                out_anexo_conteudo.append(self.processos[0]['andamentos'][i]['anexo']['conteudo'])        \n",
    "\n",
    "        arr=[[out_data,out_descricao,out_anexo_label,out_anexo_conteudo]]\n",
    "        df_andamentos = pd.DataFrame(arr,columns = ['dt_andamentos','andamentos','anexo_label','anexo_conteudo'])\n",
    "        \n",
    "        return df_andamentos\n",
    "    \n",
    "    def prep_peticoes(self):\n",
    "        if processos[0]['peticoes'] is None:\n",
    "            data = []#{}\n",
    "            tipo = []#{}\n",
    "        else:\n",
    "            a=np.array(self.processos[0]['peticoes'])\n",
    "            a.size\n",
    "            data = []\n",
    "            tipo = []\n",
    "            for i in range(a.size):\n",
    "                data.append(self.processos[0]['peticoes'][i]['data'])\n",
    "                tipo.append(self.processos[0]['peticoes'][i]['tipo'])\n",
    "            \n",
    "        arr=[[data, tipo]]\n",
    "        df_peticoes = pd.DataFrame(arr,columns = ['dt_peticoes','peticoes_tipo'])\n",
    "        return df_peticoes\n",
    "    \n",
    "    def prep_publicacoes(self):\n",
    "        a=np.array(self.publicacoes)\n",
    "        total_publicacoes = a.size\n",
    "        \n",
    "        data = []\n",
    "        numeroInstancia = []\n",
    "        conteudo = []\n",
    "        numeroCNJ = []\n",
    "        numeroAntigo = []\n",
    "        numeroUnificado = []\n",
    "        numeroInstancia = []\n",
    "        anoPublicacao = []\n",
    "        dataPublicacao = []\n",
    "        diario = []\n",
    "        diarioUF = []\n",
    "        cidadeComarcaDescricao = []\n",
    "        varaDescricao  = []\n",
    "        arquivada = []\n",
    "        complemento = []\n",
    "        conteudo = []\n",
    "        despacho = []\n",
    "        #Empresa\n",
    "        codVinculo = []\n",
    "        vinculo = []\n",
    "        #Busca\n",
    "        codTermo = []\n",
    "        termoEncontrado = []\n",
    "        buscaLote = []\n",
    "        buscaLoteAno = []\n",
    "        buscaLoteMes = []\n",
    "        buscaLoteGrupo = []\n",
    "        #Correcoes\n",
    "        corrigido = []\n",
    "        corrigidoCidade = []\n",
    "        corrigidoConteudo = []\n",
    "        corrigidoDespacho = []\n",
    "        corrigidoNumero = []\n",
    "        corrigidoOrgao = []\n",
    "        corrigidoVara = []\n",
    "        conferido = []\n",
    "        for i in range(a.size):\n",
    "            data.append(self.publicacoes[i]['dataPublicacao'])\n",
    "            numeroInstancia.append(self.publicacoes[i]['numeroInstancia'])\n",
    "            conteudo.append(self.publicacoes[i]['conteudo'])\n",
    "            \n",
    "            numeroCNJ.append(self.publicacoes[i]['numeroCNJ'])\n",
    "            numeroAntigo.append(self.publicacoes[i]['numeroAntigo'])\n",
    "            numeroUnificado.append(self.publicacoes[i]['numeroUnificado'])\n",
    "            anoPublicacao.append(self.publicacoes[i]['anoPublicacao'])\n",
    "            dataPublicacao.append(self.publicacoes[i]['dataPublicacao'])\n",
    "            diario.append(self.publicacoes[i]['diario'])\n",
    "            diarioUF.append(self.publicacoes[i]['diarioUF'])\n",
    "            cidadeComarcaDescricao.append(self.publicacoes[i]['cidadeComarcaDescricao'])\n",
    "            varaDescricao.append(self.publicacoes[i]['varaDescricao'])\n",
    "            arquivada.append(self.publicacoes[i]['arquivada'])\n",
    "            complemento.append(self.publicacoes[i]['complemento'])\n",
    "            despacho.append(self.publicacoes[i]['despacho'])\n",
    "            #Empresa\n",
    "            codVinculo.append(self.publicacoes[i]['codVinculo'])\n",
    "            vinculo.append(self.publicacoes[i]['vinculo'])\n",
    "            #Busca\n",
    "            codTermo.append(self.publicacoes[i]['codTermo'])\n",
    "            termoEncontrado.append(self.publicacoes[i]['termoEncontrado'])\n",
    "            buscaLote.append(self.publicacoes[i]['buscaLote'])\n",
    "            buscaLoteAno.append(self.publicacoes[i]['buscaLoteAno'])\n",
    "            buscaLoteMes.append(self.publicacoes[i]['buscaLoteMes'])\n",
    "            buscaLoteGrupo.append(self.publicacoes[i]['buscaLoteGrupo'])\n",
    "            #Correcoes\n",
    "            corrigido.append(self.publicacoes[i]['corrigido'])\n",
    "            corrigidoCidade.append(self.publicacoes[i]['corrigidoCidade'])\n",
    "            corrigidoConteudo.append(self.publicacoes[i]['corrigidoConteudo'])\n",
    "            corrigidoDespacho.append(self.publicacoes[i]['corrigidoDespacho'])\n",
    "            corrigidoNumero.append(self.publicacoes[i]['corrigidoNumero'])\n",
    "            corrigidoOrgao.append(self.publicacoes[i]['corrigidoOrgao'])\n",
    "            corrigidoVara.append(self.publicacoes[i]['corrigidoVara'])\n",
    "            conferido.append(self.publicacoes[i]['conferido'])\n",
    "\n",
    "        arr=[[total_publicacoes,data,numeroInstancia,conteudo,numeroCNJ,numeroAntigo,numeroUnificado,numeroInstancia,anoPublicacao,dataPublicacao,diario,diarioUF,cidadeComarcaDescricao,varaDescricao,arquivada,complemento,despacho,codVinculo,vinculo,codTermo,termoEncontrado,buscaLote,buscaLoteAno,buscaLoteMes,buscaLoteGrupo,corrigido,corrigidoCidade,corrigidoConteudo,corrigidoDespacho,corrigidoNumero,corrigidoOrgao,corrigidoVara,conferido]]\n",
    "        df_publicacoes = pd.DataFrame(arr,columns = ['total_publicacoes','data','numeroInstancia','conteudo','numeroCNJ','numeroAntigo','numeroUnificado','numeroInstancia','anoPublicacao','dataPublicacao','diario','diarioUF','cidadeComarcaDescricao','varaDescricao','arquivada','complemento','despacho','codVinculo','vinculo','codTermo','termoEncontrado','buscaLote','buscaLoteAno','buscaLoteMes','buscaLoteGrupo','corrigido','corrigidoCidade','corrigidoConteudo','corrigidoDespacho','corrigidoNumero','corrigidoOrgao','corrigidoVara','conferido'])\n",
    "        \n",
    "        return df_publicacoes\n",
    "     \n",
    "    def dq_msg(self, val):\n",
    "        if val != None:\n",
    "            return val\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    def text_process(self, mess):\n",
    "        \"\"\"\n",
    "        Takes in a string of text, then performs the following:\n",
    "        1. Remove all punctuation\n",
    "        2. Remove all stopwords\n",
    "        3. Returns a list of the cleaned text\n",
    "        \"\"\"\n",
    "        # Check characters to see if they are in punctuation\n",
    "        nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "        # Join the characters again to form the string.\n",
    "        nopunc = ''.join(nopunc)\n",
    "\n",
    "        # Now just remove any stopwords\n",
    "        return [word for word in nopunc.split() if word.lower() not in stopwords.words('portuguese')]\n",
    "\n",
    "    def qd_text(self, tag, mess, padrao):\n",
    "        if padrao != '': #Caso em que a mensagem e tag são separados por 3 espaços\n",
    "            quebra = mess.split(padrao)\n",
    "            mess = quebra[1:]\n",
    "            tag = quebra[0]\n",
    "        elif padrao == '': #Caso em que há um atributo para tag e outro para a mensagem         \n",
    "            if type(mess) == dict:\n",
    "                tag = mess['label']\n",
    "                mess = mess['conteudo']\n",
    "\n",
    "        return tag, mess\n",
    "\n",
    "\n",
    "    def prep_tags(self, atr):\n",
    "        num = []\n",
    "        n = []\n",
    "        andamentos = []\n",
    "        label = []\n",
    "        dt = []\n",
    "        for i in range(len(self.big_table[atr['texto']])):\n",
    "            a = self.big_table[atr['texto']][i]\n",
    "            l = self.big_table[atr['label']][i]\n",
    "            d = self.big_table[atr['data']][i]\n",
    "            for j in range(len(a)):\n",
    "                n = n + [self.big_table[atr['id']][i]]\n",
    "\n",
    "            num = num + n\n",
    "            n = []\n",
    "            andamentos = andamentos + a\n",
    "            label = label + l\n",
    "            dt = dt + d\n",
    "\n",
    "        arr_andamentos = np.array(andamentos)\n",
    "        arr_label = np.array(label)\n",
    "\n",
    "        messages = []\n",
    "        tags = []\n",
    "        for i in range(arr_andamentos.size):\n",
    "            tag = arr_label[i]\n",
    "            mess = arr_andamentos[i]\n",
    "\n",
    "            tag, mess = qd_text(tag, mess, atr['padrao'])\n",
    "\n",
    "            messages.append(mess) \n",
    "            tags.append(tag)\n",
    "\n",
    "        return num, dt, tags, messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objeto Proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"data/itau\"\n",
    "filelist = os.listdir(path_data)\n",
    "big_table = pd.DataFrame()\n",
    "n = len(filelist)\n",
    "#n=2\n",
    "for i in range(n):\n",
    "    #i=4\n",
    "    arquivo_json = open(path_data+'/'+filelist[i],'r')\n",
    "    dados_json = json.load(arquivo_json)\n",
    "    \n",
    "    #Grandes categorias\n",
    "    numero = dados_json['numero']\n",
    "    processos = dados_json['processos']\n",
    "    publicacoes = dados_json['publicacoes']\n",
    "    \n",
    "    proc = Prep(numero, processos, publicacoes)\n",
    "    df_numero = proc.prep_num_processo()\n",
    "    df_base = proc.prep_base()\n",
    "    df_partes = proc.prep_partes()\n",
    "    df_advogados = proc.prep_advogados()\n",
    "    df_andamentos = proc.prep_andamentos()\n",
    "    df_peticoes = proc.prep_peticoes()\n",
    "    df_publicacoes = proc.prep_publicacoes()\n",
    "    df = pd.concat([df_numero,df_base,df_partes,df_advogados,df_andamentos,df_peticoes,df_publicacoes],axis=1)\n",
    "    big_table = pd.concat([big_table,df],axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.big_table = big_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup do atributo de texto a ser analisado\n",
    "atr = [{'id':'id', 'data':'dt_andamentos','label':'andamentos', 'texto':'andamentos','padrao':'   '},#Andamentos [0]\n",
    "       {'id':'id', 'data':'dt_andamentos','label':'anexo_label', 'texto':'anexo_conteudo','padrao':''},#Anexos [1]\n",
    "       {'id':'id', 'data':'dt_peticoes','label':'peticoes_tipo', 'texto':'peticoes_tipo','padrao':''},#Peticoes [2]\n",
    "       {'id':'id', 'data':'data','label':'conteudo', 'texto':'conteudo','padrao':''},#Publicacoes [3]\n",
    "       {'id':'id', 'data':'data','label':'conteudo', 'texto':'despacho','padrao':''}]#Despachos [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt = ['bt_andamentos','bt_anexos','bt_peticoes','bt_publicacoes','bt_despachos']\n",
    "atributos = atr[2]\n",
    "num, dt, tags, messages = proc.prep_tags(atributos)\n",
    "\n",
    "df_num = pd.DataFrame(num,columns = ['num'])\n",
    "df_dt = pd.DataFrame(dt,columns = ['dt'])\n",
    "df_tags = pd.DataFrame(tags,columns = ['tags'])\n",
    "df_messages = pd.DataFrame(messages, columns=['messages'])\n",
    "\n",
    "bt = pd.concat([df_num, df_dt, df_tags,df_messages],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-370-8ef66c3638d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'q'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'eval'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.CountVectorizer"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = CountVectorizer(analyzer=proc.text_process).fit(df_descritivo['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer().fit(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exploração das Tags e Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos = atr[2] #Selecione o atributo a ser analisado\n",
    "num, dt, tags, messages = proc.prep_tags(atributos)\n",
    "\n",
    "df_tags = pd.DataFrame(tags,columns = ['tags'])\n",
    "df_messages = pd.DataFrame(messages, columns=['messages'])\n",
    "bt = pd.concat([df_tags,df_messages],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_descritivo = bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [Petição, Intermediária]\n",
       "1    [Contrarrazões, Apelação]\n",
       "2     [Petição, Intermediária]\n",
       "3     [Petição, Intermediária]\n",
       "4     [Petição, Intermediária]\n",
       "Name: messages, dtype: object"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b64999c198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Impressão das mensagens e enumeração\n",
    "#for message_no, message in enumerate(messages[:10]):\n",
    "#    print(message_no, message)\n",
    "#    print('\\n')\n",
    "#Algumas estatísticas descritivas básicas das tags e messages\n",
    "#df_descritivo.describe()\n",
    "#Descrição das mensagens, agrupadas por tags\n",
    "df_descritivo.groupby('tags').describe()\n",
    "#Qualificação das mensagens de andamentos\n",
    "df_descritivo['messages'] = df_descritivo['messages'].apply(proc.dq_msg)\n",
    "df_descritivo['length'] = df_descritivo['messages'].apply(len)\n",
    "#Distribuição do comprimento das mensagens de andamentos\n",
    "df_descritivo['length'].plot(kind='hist', bins=50)\n",
    "#Estatísticas descritivas do comprimentos das mensagens de andamentos\n",
    "df_descritivo['length'].describe() \n",
    "#Análise da mensagem com maior numero de termos\n",
    "df_descritivo.sort_values(['length'],ascending=False)[1:100]\n",
    "#Analise de alguns comprimentos de mensagens relevantes\n",
    "#length_msg = [15115, 14912, 12222, 12086, 11617, 11458, 11408, 11270, 11089, 11051] \n",
    "#for i in range(len(length_msg)):\n",
    "#    print(df_descritivo[df_descritivo['length'] == length_msg[i]]['tags'].iloc[0])\n",
    "#    #print(df_descritivo[df_descritivo['length'] == length_msg[i]]['messages'].iloc[0])\n",
    "#Analise da distribuição do comprimento das mensagens de uma tag específica\n",
    "#tag_especifica = ['Remetido ao DJE', 'Decisão Proferida','Julgada Improcedente a Ação - Sentença Completa']\n",
    "#for i in range(len(tag_especifica)):\n",
    "#    df_descritivo[df_descritivo['tags'] == tag_especifica[i]].hist(column='length', by='tags', bins=50,figsize=(12,4))\n",
    "#Visualização das stopwords em portugues\n",
    "df_sw = pd.DataFrame(stopwords.words('portuguese'),columns=['stopwords']) # Cuidado o NÃO é uma stopword\n",
    "df_sw.head()\n",
    "#Visualizacao das tags e mensagens originais\n",
    "df_descritivo.head()\n",
    "#Visualizacao das mensagens \"tokenized\"\n",
    "df_descritivo['messages'].head(5).apply(proc.text_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploração da BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(analyzer=proc.text_process).fit(df_descritivo['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674\n",
      "  (0, 207)\t1\n",
      "  (0, 276)\t1\n",
      "Shape of Sparse Matrix:  (2255, 674)\n",
      "Amount of Non-Zero occurences:  5794\n"
     ]
    }
   ],
   "source": [
    "#Quantidade de termos no dicionario\n",
    "print(len(bow.vocabulary_)) #Numero de termos no vocabulario\n",
    "#Termos do dicionario\n",
    "bow.vocabulary_\n",
    "#Selecao de uma message específica\n",
    "m1 = df_descritivo['messages'][0]\n",
    "#Veja a BOW da message\n",
    "bow1 = bow.transform([m1])\n",
    "print(bow1)\n",
    "#Selecione o termo de maior frequencia\n",
    "#print(bow.get_feature_names()[35934])\n",
    "#Analise de termos relevantes\n",
    "termos = bow.get_feature_names()\n",
    "df_termos = pd.DataFrame(termos,columns=['termos'])\n",
    "df_termos[df_termos['termos'].str.contains('valor')]\n",
    "#Transformacao da BOW em um DataFrame esparso completo de mensagens\n",
    "df_bow = bow.transform(df_descritivo['messages'])\n",
    "#Algumas informações técnicas da BOW\n",
    "print('Shape of Sparse Matrix: ', df_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', df_bow.nnz)\n",
    "sparsity = (100.0 * df_bow.nnz / (df_bow.shape[0] * df_bow.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peso e Normalização com TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploração da TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer().fit(df_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 473)\t0.707106781187\n",
      "  (0, 356)\t0.707106781187\n",
      "624\n",
      "  (0, 473)\t0.707106781187\n",
      "  (0, 356)\t0.707106781187\n",
      "  (1, 473)\t0.707106781187\n",
      "  (1, 356)\t0.707106781187\n",
      "  (2, 633)\t0.707106781187\n",
      "  (2, 116)\t0.707106781187\n",
      "  (3, 473)\t0.707106781187\n",
      "  (3, 356)\t0.707106781187\n",
      "  (4, 473)\t0.707106781187\n",
      "  (4, 356)\t0.707106781187\n",
      "  (5, 473)\t0.707106781187\n",
      "  (5, 356)\t0.707106781187\n",
      "  (6, 473)\t0.707106781187\n",
      "  (6, 356)\t0.707106781187\n",
      "  (7, 473)\t0.707106781187\n",
      "  (7, 356)\t0.707106781187\n",
      "  (8, 669)\t0.0572384267467\n",
      "  (8, 658)\t0.0760925437208\n",
      "  (8, 652)\t0.0819763170562\n",
      "  (8, 647)\t0.0760925437208\n",
      "  (8, 646)\t0.0660341667681\n",
      "  (8, 640)\t0.0572384267467\n",
      "  (8, 623)\t0.0686798659735\n",
      "  (8, 607)\t0.0660341667681\n",
      "  (8, 602)\t0.0819763170562\n",
      "  :\t:\n",
      "  (195, 116)\t0.707106781187\n",
      "  (196, 633)\t0.707106781187\n",
      "  (196, 116)\t0.707106781187\n",
      "  (197, 633)\t0.707106781187\n",
      "  (197, 116)\t0.707106781187\n",
      "  (198, 633)\t0.707106781187\n",
      "  (198, 116)\t0.707106781187\n",
      "  (199, 633)\t0.707106781187\n",
      "  (199, 116)\t0.707106781187\n",
      "  (200, 633)\t0.707106781187\n",
      "  (200, 116)\t0.707106781187\n",
      "  (201, 633)\t0.707106781187\n",
      "  (201, 116)\t0.707106781187\n",
      "  (202, 633)\t0.707106781187\n",
      "  (202, 116)\t0.707106781187\n",
      "  (203, 633)\t0.707106781187\n",
      "  (203, 116)\t0.707106781187\n",
      "  (204, 633)\t0.707106781187\n",
      "  (204, 116)\t0.707106781187\n",
      "  (205, 633)\t0.707106781187\n",
      "  (205, 116)\t0.707106781187\n",
      "  (206, 633)\t0.707106781187\n",
      "  (206, 116)\t0.707106781187\n",
      "  (207, 633)\t0.707106781187\n",
      "  (207, 116)\t0.707106781187\n"
     ]
    }
   ],
   "source": [
    "#Pesos dos termos em uma mensagem, usando TF-IDF \n",
    "tfidf1 = tfidf.transform(bow1)\n",
    "print(tfidf1)\n",
    "#Verificar apenas a IDF de termos relevantes\n",
    "#tfidf.idf_[bow.vocabulary_['valor']]\n",
    "#Transforma as mensagens do TD_BOW em mensagens do DF_TFIDF\n",
    "df_tfidf = tfidf.transform(df_bow)\n",
    "#Analise da TF-IDF\n",
    "print(df_descritivo.size)\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
