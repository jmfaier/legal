{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from xml.dom import minidom\n",
    "#arquivo = open('data/cpfl/2018/Publicações  PARCERIA TERADATA AnoPublicação 2018 arquivo 1-3.xml','r')\n",
    "#xmldoc = minidom.parse(arquivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe Preparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prep:\n",
    "    \n",
    "    def __init__(self,numero,processos,publicacoes):\n",
    "        self.numero = numero\n",
    "        self.processos = processos\n",
    "        self.publicacoes = publicacoes\n",
    "        self.big_table = big_table\n",
    "        #self.val = None\n",
    "        \n",
    "    def prep_num_processo(self):\n",
    "        proc = self.numero #Fonte: andamentos\n",
    "        id_proc = np.array([self.numero]) #Fonte: cabeçalho\n",
    "        \n",
    "        #Numero do processo - Fonte: cabeçalho\n",
    "        df_id = pd.DataFrame(id_proc,columns=['id'])\n",
    "        \n",
    "        #Quebra do numero do processo - Fonte: andamentos\n",
    "        p = proc.split('.')\n",
    "\n",
    "        num = int(p[0].split('-')[0]+p[0].split('-')[1])\n",
    "        ano_inicio = int(p[1])\n",
    "        ramo = int(p[2])\n",
    "        tribunal = int(p[3])\n",
    "        vara_orig = int(p[4])\n",
    "\n",
    "        arr = np.array([[num, ano_inicio, ramo, tribunal, vara_orig]])\n",
    "\n",
    "        df_proc = pd.DataFrame(arr,columns=['num_seq','ano_inicio','ramo','tribunal','vara_orig'])\n",
    "        \n",
    "        #Concatenacao\n",
    "        df = pd.concat([df_id, df_proc],axis=1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prep_base(self):\n",
    "        classe = self.processos[0]['classe']\n",
    "        classe_area = self.processos[0]['classe_area']\n",
    "        assunto = self.processos[0]['assunto']\n",
    "        outros_assuntos = self.processos[0]['outros_assuntos']\n",
    "        juiz = self.processos[0]['juiz']\n",
    "        valor = self.processos[0]['valor']\n",
    "        audiencias = self.processos[0]['audiencias']\n",
    "        incidentes = self.processos[0]['audiencias']\n",
    "        total_andamentos = self.processos[0]['total_andamentos']\n",
    "\n",
    "        df_base = pd.DataFrame(np.array([[classe, classe_area, assunto, outros_assuntos, juiz, audiencias, incidentes,valor,total_andamentos]]), columns=['classe', 'classe_area', 'assunto', 'outros_assuntos', 'juiz', 'audiencias', 'incidentes','valor','total_andamentos'])\n",
    "        \n",
    "        return df_base\n",
    "    \n",
    "    def prep_partes(self):\n",
    "        a=np.array(self.processos[0]['partes'])\n",
    "        a.size\n",
    "        out_tipo = []\n",
    "        out_reqte= []\n",
    "        out_reqda = []\n",
    "        for i in range(a.size):\n",
    "            tipo = self.processos[0]['partes'][i]['tipo']\n",
    "            if tipo == 'Reqte' or tipo == 'Autor' or tipo == 'Exeqte':\n",
    "                out_reqte.append(self.processos[0]['partes'][i]['nome'])\n",
    "            elif tipo == 'Reqda' or tipo == 'Reqdo' or tipo == 'Ré' or tipo == 'Réu' or tipo == 'Exectdo' or tipo == 'TerIntCer':\n",
    "                out_reqda.append(self.processos[0]['partes'][i]['nome'])\n",
    "        arr=[[out_reqte, out_reqda]]\n",
    "        df_partes = pd.DataFrame(arr,columns = ['Reqte','Reqdo'])\n",
    "        \n",
    "        return df_partes\n",
    "    \n",
    "    def prep_advogados(self):\n",
    "        a=np.array(self.processos[0]['advogados'])\n",
    "        a.size\n",
    "        out_advogados = []\n",
    "        for i in range(a.size):\n",
    "            out_advogados.append(self.processos[0]['advogados'][i]['nome'])\n",
    "        arr=[[out_advogados]]\n",
    "        df_advogados = pd.DataFrame(arr,columns = ['Advogados'])\n",
    "        \n",
    "        return df_advogados\n",
    "    \n",
    "    def prep_andamentos(self):\n",
    "        a=np.array(self.processos[0]['andamentos'])\n",
    "        a.size\n",
    "        out_data = []\n",
    "        out_descricao = []\n",
    "        out_anexo = []\n",
    "        out_anexo_label = []\n",
    "        out_anexo_conteudo = []\n",
    "        for i in range(a.size):\n",
    "            out_data.append(self.processos[0]['andamentos'][i]['data'])\n",
    "            out_descricao.append(self.processos[0]['andamentos'][i]['descricao'])\n",
    "            if processos[0]['andamentos'][i]['anexo'] == 'Não existe anexo':\n",
    "                out_anexo_label.append(self.processos[0]['andamentos'][0]['anexo'])\n",
    "                out_anexo_conteudo.append(self.processos[0]['andamentos'][0]['anexo'])\n",
    "            else:\n",
    "                out_anexo_label.append(self.processos[0]['andamentos'][i]['anexo']['label'])\n",
    "                out_anexo_conteudo.append(self.processos[0]['andamentos'][i]['anexo']['conteudo'])        \n",
    "\n",
    "        arr=[[out_data,out_descricao,out_anexo_label,out_anexo_conteudo]]\n",
    "        df_andamentos = pd.DataFrame(arr,columns = ['dt_andamentos','andamentos','anexo_label','anexo_conteudo'])\n",
    "        \n",
    "        return df_andamentos\n",
    "    \n",
    "    def prep_peticoes(self):\n",
    "        if processos[0]['peticoes'] is None:\n",
    "            data = {}\n",
    "            tipo = {}\n",
    "        else:\n",
    "            a=np.array(self.processos[0]['peticoes'])\n",
    "            a.size\n",
    "            data = []\n",
    "            tipo = []\n",
    "            for i in range(a.size):\n",
    "                data.append(self.processos[0]['peticoes'][i]['data'])\n",
    "                tipo.append(self.processos[0]['peticoes'][i]['tipo'])\n",
    "            \n",
    "        arr=[[data, tipo]]\n",
    "        df_peticoes = pd.DataFrame(arr,columns = ['dt_peticoes','peticoes_tipo'])\n",
    "        return df_peticoes\n",
    "    \n",
    "    def prep_publicacoes(self):\n",
    "        a=np.array(self.publicacoes)\n",
    "        total_publicacoes = a.size\n",
    "        \n",
    "        data = []\n",
    "        numeroInstancia = []\n",
    "        conteudo = []\n",
    "        numeroCNJ = []\n",
    "        numeroAntigo = []\n",
    "        numeroUnificado = []\n",
    "        numeroInstancia = []\n",
    "        anoPublicacao = []\n",
    "        dataPublicacao = []\n",
    "        diario = []\n",
    "        diarioUF = []\n",
    "        cidadeComarcaDescricao = []\n",
    "        varaDescricao  = []\n",
    "        arquivada = []\n",
    "        complemento = []\n",
    "        conteudo = []\n",
    "        despacho = []\n",
    "        #Empresa\n",
    "        codVinculo = []\n",
    "        vinculo = []\n",
    "        #Busca\n",
    "        codTermo = []\n",
    "        termoEncontrado = []\n",
    "        buscaLote = []\n",
    "        buscaLoteAno = []\n",
    "        buscaLoteMes = []\n",
    "        buscaLoteGrupo = []\n",
    "        #Correcoes\n",
    "        corrigido = []\n",
    "        corrigidoCidade = []\n",
    "        corrigidoConteudo = []\n",
    "        corrigidoDespacho = []\n",
    "        corrigidoNumero = []\n",
    "        corrigidoOrgao = []\n",
    "        corrigidoVara = []\n",
    "        conferido = []\n",
    "        for i in range(a.size):\n",
    "            data.append(self.publicacoes[i]['dataPublicacao'])\n",
    "            numeroInstancia.append(self.publicacoes[i]['numeroInstancia'])\n",
    "            conteudo.append(self.publicacoes[i]['conteudo'])\n",
    "            \n",
    "            numeroCNJ.append(self.publicacoes[i]['numeroCNJ'])\n",
    "            numeroAntigo.append(self.publicacoes[i]['numeroAntigo'])\n",
    "            numeroUnificado.append(self.publicacoes[i]['numeroUnificado'])\n",
    "            anoPublicacao.append(self.publicacoes[i]['anoPublicacao'])\n",
    "            dataPublicacao.append(self.publicacoes[i]['dataPublicacao'])\n",
    "            diario.append(self.publicacoes[i]['diario'])\n",
    "            diarioUF.append(self.publicacoes[i]['diarioUF'])\n",
    "            cidadeComarcaDescricao.append(self.publicacoes[i]['cidadeComarcaDescricao'])\n",
    "            varaDescricao.append(self.publicacoes[i]['varaDescricao'])\n",
    "            arquivada.append(self.publicacoes[i]['arquivada'])\n",
    "            complemento.append(self.publicacoes[i]['complemento'])\n",
    "            despacho.append(self.publicacoes[i]['despacho'])\n",
    "            #Empresa\n",
    "            codVinculo.append(self.publicacoes[i]['codVinculo'])\n",
    "            vinculo.append(self.publicacoes[i]['vinculo'])\n",
    "            #Busca\n",
    "            codTermo.append(self.publicacoes[i]['codTermo'])\n",
    "            termoEncontrado.append(self.publicacoes[i]['termoEncontrado'])\n",
    "            buscaLote.append(self.publicacoes[i]['buscaLote'])\n",
    "            buscaLoteAno.append(self.publicacoes[i]['buscaLoteAno'])\n",
    "            buscaLoteMes.append(self.publicacoes[i]['buscaLoteMes'])\n",
    "            buscaLoteGrupo.append(self.publicacoes[i]['buscaLoteGrupo'])\n",
    "            #Correcoes\n",
    "            corrigido.append(self.publicacoes[i]['corrigido'])\n",
    "            corrigidoCidade.append(self.publicacoes[i]['corrigidoCidade'])\n",
    "            corrigidoConteudo.append(self.publicacoes[i]['corrigidoConteudo'])\n",
    "            corrigidoDespacho.append(self.publicacoes[i]['corrigidoDespacho'])\n",
    "            corrigidoNumero.append(self.publicacoes[i]['corrigidoNumero'])\n",
    "            corrigidoOrgao.append(self.publicacoes[i]['corrigidoOrgao'])\n",
    "            corrigidoVara.append(self.publicacoes[i]['corrigidoVara'])\n",
    "            conferido.append(self.publicacoes[i]['conferido'])\n",
    "\n",
    "        arr=[[total_publicacoes,data,numeroInstancia,conteudo,numeroCNJ,numeroAntigo,numeroUnificado,numeroInstancia,anoPublicacao,dataPublicacao,diario,diarioUF,cidadeComarcaDescricao,varaDescricao,arquivada,complemento,despacho,codVinculo,vinculo,codTermo,termoEncontrado,buscaLote,buscaLoteAno,buscaLoteMes,buscaLoteGrupo,corrigido,corrigidoCidade,corrigidoConteudo,corrigidoDespacho,corrigidoNumero,corrigidoOrgao,corrigidoVara,conferido]]\n",
    "        df_publicacoes = pd.DataFrame(arr,columns = ['total_publicacoes','data','numeroInstancia','conteudo','numeroCNJ','numeroAntigo','numeroUnificado','numeroInstancia','anoPublicacao','dataPublicacao','diario','diarioUF','cidadeComarcaDescricao','varaDescricao','arquivada','complemento','despacho','codVinculo','vinculo','codTermo','termoEncontrado','buscaLote','buscaLoteAno','buscaLoteMes','buscaLoteGrupo','corrigido','corrigidoCidade','corrigidoConteudo','corrigidoDespacho','corrigidoNumero','corrigidoOrgao','corrigidoVara','conferido'])\n",
    "        \n",
    "        return df_publicacoes\n",
    "    \n",
    "    def prep_tags(self,atr):\n",
    "        num = []\n",
    "        n = []\n",
    "        andamentos = []\n",
    "        label = []\n",
    "        dt_andamentos = []\n",
    "        for i in range(len(self.big_table[atr['texto']])):\n",
    "            a = self.big_table[atr['texto']][i]\n",
    "            l = self.big_table[atr['label']][i]\n",
    "            d = self.big_table[atr['data']][i]\n",
    "            for j in range(len(a)):\n",
    "                n = n + [self.big_table[atr['id']][i]]\n",
    "\n",
    "            num = num + n\n",
    "            n = []\n",
    "            andamentos = andamentos + a\n",
    "            label = label + l\n",
    "            dt_andamentos = dt_andamentos + d\n",
    "\n",
    "        arr_andamentos = np.array(andamentos)\n",
    "        arr_label = np.array(label)\n",
    "\n",
    "        messages = []\n",
    "        tags = []\n",
    "        for i in range(arr_andamentos.size):\n",
    "            tag = arr_label[i]\n",
    "            mess = arr_andamentos[i]\n",
    "\n",
    "            if atr['padrao'] == '': #Caso em que há um atributo para tag e outro para a mensagem\n",
    "                messages.append(mess)\n",
    "                tags.append(tag)\n",
    "            elif atr['padrao'] != '': #Caso em que a mensagem e tag são separados por 3 espaços\n",
    "                quebra = mess.split(atr['padrao'])\n",
    "                messages.append(quebra[1:])\n",
    "                tags.append(quebra[0])\n",
    "\n",
    "        return tags, messages\n",
    "    \n",
    "    def dq_msg(self, val):\n",
    "        if val != None:\n",
    "            return val\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    def text_process(self, mess):\n",
    "        \"\"\"\n",
    "        Takes in a string of text, then performs the following:\n",
    "        1. Remove all punctuation\n",
    "        2. Remove all stopwords\n",
    "        3. Returns a list of the cleaned text\n",
    "        \"\"\"\n",
    "        # Check characters to see if they are in punctuation\n",
    "        nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "        # Join the characters again to form the string.\n",
    "        nopunc = ''.join(nopunc)\n",
    "\n",
    "        # Now just remove any stopwords\n",
    "        return [word for word in nopunc.split() if word.lower() not in stopwords.words('portuguese')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"data\"\n",
    "filelist = os.listdir(path_data)\n",
    "big_table = pd.DataFrame()\n",
    "#n = len(filelist)-1\n",
    "n=2\n",
    "for i in range(n):\n",
    "    #i=4\n",
    "    arquivo_json = open(path_data+'/'+filelist[i],'r')\n",
    "    dados_json = json.load(arquivo_json)\n",
    "    \n",
    "    #Grandes categorias\n",
    "    numero = dados_json['numero']\n",
    "    processos = dados_json['processos']\n",
    "    publicacoes = dados_json['publicacoes']\n",
    "    \n",
    "    proc = Prep(numero, processos, publicacoes)\n",
    "    df_numero = proc.prep_num_processo()\n",
    "    df_base = proc.prep_base()\n",
    "    df_partes = proc.prep_partes()\n",
    "    df_advogados = proc.prep_advogados()\n",
    "    df_andamentos = proc.prep_andamentos()\n",
    "    df_peticoes = proc.prep_peticoes()\n",
    "    df_publicacoes = proc.prep_publicacoes()\n",
    "    df = pd.concat([df_numero,df_base,df_partes,df_advogados,df_andamentos,df_peticoes,df_publicacoes],axis=1)\n",
    "    big_table = pd.concat([big_table,df],axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.big_table = big_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qd_text(tag, mess, padrao):\n",
    "    if padrao != '': #Caso em que há um atributo para tag e outro para a mensagem\n",
    "        quebra = mess.split(atr['padrao'])\n",
    "        messages.append(quebra[1:])\n",
    "        tags.append(quebra[0])\n",
    "    elif padrao == '': #Caso em que a mensagem e tag são separados por 3 espaços         \n",
    "        if type(mess) == dict:\n",
    "            tag = mess['label']\n",
    "            #mess = mess['conteudo']\n",
    "        messages.append(mess) \n",
    "        tags.append(tag)\n",
    "    \n",
    "    return tags, messages\n",
    "    \n",
    "\n",
    "def prep_tags(atr):\n",
    "    num = []\n",
    "    n = []\n",
    "    andamentos = []\n",
    "    label = []\n",
    "    dt_andamentos = []\n",
    "    for i in range(len(big_table[atr['texto']])):\n",
    "        a = big_table[atr['texto']][i]\n",
    "        l = big_table[atr['label']][i]\n",
    "        d = big_table[atr['data']][i]\n",
    "        for j in range(len(a)):\n",
    "            n = n + [big_table[atr['id']][i]]\n",
    "\n",
    "        num = num + n\n",
    "        n = []\n",
    "        andamentos = andamentos + a\n",
    "        label = label + l\n",
    "        dt_andamentos = dt_andamentos + d\n",
    "\n",
    "    arr_andamentos = np.array(andamentos)\n",
    "    arr_label = np.array(label)\n",
    "\n",
    "    messages = []\n",
    "    tags = []\n",
    "    for i in range(arr_andamentos.size):\n",
    "        tag = arr_label[i]\n",
    "        mess = arr_andamentos[i]\n",
    "            \n",
    "        tags, messages = qd_text(tag, mess, atr['padrao'])\n",
    "        print(tags)\n",
    "        input(\"Press Enter to continue...\")\n",
    "\n",
    "    return tags, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup do atributo de texto a ser analisado\n",
    "#Andamentos\n",
    "atr = [{'id':'id', 'data':'dt_andamentos','label':'andamentos', 'texto':'andamentos','padrao':'   '},#Andamentos\n",
    "       {'id':'id', 'data':'dt_andamentos','label':'anexo_label', 'texto':'anexo_conteudo','padrao':''},#Anexos\n",
    "       {'id':'id', 'data':'dt_peticoes','label':'peticoes_tipo', 'texto':'peticoes_tipo','padrao':''},#Peticoes\n",
    "       {'id':'id', 'data':'data','label':'conteudo', 'texto':'conteudo','padrao':''},#Publicacoes\n",
    "       {'id':'id', 'data':'data','label':'conteudo', 'texto':'despacho','padrao':''}]#Despachos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags, messages = prep_tags(atr[1])\n",
    "tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploração das Tags e Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-276-81911dfef475>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#df_descritivo.describe()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#Descrição das mensagens, agrupadas por tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdf_descritivo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tags'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m#Qualificação das mensagens de andamentos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdf_descritivo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'messages'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_descritivo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'messages'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdq_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\curso-luiza\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36mdescribe\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_group_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1311\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\curso-luiza\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    803\u001b[0m         \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mode.chained_assignment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\curso-luiza\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m         keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n\u001b[1;32m--> 809\u001b[1;33m                                                    self.axis)\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\curso-luiza\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m   1943\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1944\u001b[0m         \u001b[0mmutated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmutated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1945\u001b[1;33m         \u001b[0msplitter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_splitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1946\u001b[0m         \u001b[0mgroup_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_group_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\curso-luiza\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36m_get_splitter\u001b[1;34m(self, data, axis)\u001b[0m\n\u001b[0;32m   1926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1927\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_splitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1928\u001b[1;33m         \u001b[0mcomp_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1929\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mget_splitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomp_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.cache_readonly.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\curso-luiza\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36mgroup_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2038\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgroup_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2040\u001b[1;33m         \u001b[0mcomp_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_group_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_compressed_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2042\u001b[0m         \u001b[0mngroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_group_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\curso-luiza\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36m_get_compressed_labels\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2055\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_compressed_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2056\u001b[1;33m         \u001b[0mall_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mping\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupings\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2057\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m             group_index = get_group_index(all_labels, self.shape,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\curso-luiza\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2055\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_compressed_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2056\u001b[1;33m         \u001b[0mall_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mping\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupings\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2057\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m             group_index = get_group_index(all_labels, self.shape,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\curso-luiza\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36mlabels\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2748\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2749\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2750\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2751\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\curso-luiza\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36m_make_labels\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2765\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2766\u001b[0m                 labels, uniques = algorithms.factorize(\n\u001b[1;32m-> 2767\u001b[1;33m                     self.grouper, sort=self.sort)\n\u001b[0m\u001b[0;32m   2768\u001b[0m                 \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2769\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\curso-luiza\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[1;34m(values, sort, order, na_sentinel, size_hint)\u001b[0m\n\u001b[0;32m    469\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec_klass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[0mcheck_nulls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_nulls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ensure_platform_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_labels\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "#Impressão das mensagens e enumeração\n",
    "#for message_no, message in enumerate(messages[:10]):\n",
    "#    print(message_no, message)\n",
    "#    print('\\n')\n",
    "#Algumas estatísticas descritivas básicas das tags e messages\n",
    "df_tags = pd.DataFrame(tags,columns = ['tags'])\n",
    "df_messages = pd.DataFrame(messages, columns=['messages'])\n",
    "#Criacao do df_descritivo a ser analisado\n",
    "df_descritivo = pd.concat([df_tags,df_messages],axis=1)\n",
    "#df_descritivo.describe()\n",
    "#Descrição das mensagens, agrupadas por tags\n",
    "df_descritivo.groupby('tags').describe()\n",
    "#Qualificação das mensagens de andamentos\n",
    "df_descritivo['messages'] = df_descritivo['messages'].apply(proc.dq_msg)\n",
    "df_descritivo['length'] = df_descritivo['messages'].apply(len)\n",
    "#Distribuição do comprimento das mensagens de andamentos\n",
    "df_descritivo['length'].plot(kind='hist', bins=50)\n",
    "#Estatísticas descritivas do comprimentos das mensagens de andamentos\n",
    "df_descritivo['length'].describe() \n",
    "#Análise da mensagem com maior numero de termos\n",
    "df_descritivo.sort_values(['length'],ascending=False)[1:100]\n",
    "#Analise de alguns comprimentos de mensagens relevantes\n",
    "#length_msg = [15115, 14912, 12222, 12086, 11617, 11458, 11408, 11270, 11089, 11051] \n",
    "#for i in range(len(length_msg)):\n",
    "#    print(df_descritivo[df_descritivo['length'] == length_msg[i]]['tags'].iloc[0])\n",
    "#    #print(df_descritivo[df_descritivo['length'] == length_msg[i]]['messages'].iloc[0])\n",
    "#Analise da distribuição do comprimento das mensagens de uma tag específica\n",
    "tag_especifica = ['Remetido ao DJE', 'Decisão Proferida','Julgada Improcedente a Ação - Sentença Completa']\n",
    "for i in range(len(tag_especifica)):\n",
    "    df_descritivo[df_descritivo['tags'] == tag_especifica[i]].hist(column='length', by='tags', bins=50,figsize=(12,4))\n",
    "#Visualização das stopwords em portugues\n",
    "df_sw = pd.DataFrame(stopwords.words('portuguese'),columns=['stopwords']) # Cuidado o NÃO é uma stopword\n",
    "df_sw.head()\n",
    "#Visualizacao das tags e mensagens originais\n",
    "df_descritivo.head()\n",
    "#Visualizacao das mensagens \"tokenized\"\n",
    "df_descritivo['messages'].head(5).apply(proc.text_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag de palavras (bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(analyzer=proc.text_process).fit(df_descritivo['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploração da BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697\n",
      "  (0, 12)\t1\n",
      "  (0, 59)\t1\n",
      "  (0, 82)\t1\n",
      "  (0, 91)\t1\n",
      "  (0, 103)\t1\n",
      "  (0, 211)\t2\n",
      "  (0, 224)\t1\n",
      "  (0, 227)\t1\n",
      "  (0, 302)\t1\n",
      "  (0, 329)\t1\n",
      "  (0, 330)\t1\n",
      "  (0, 345)\t1\n",
      "Shape of Sparse Matrix:  (208, 697)\n",
      "Amount of Non-Zero occurences:  2027\n"
     ]
    }
   ],
   "source": [
    "#Quantidade de termos no dicionario\n",
    "print(len(bow.vocabulary_)) #Numero de termos no vocabulario\n",
    "#Termos do dicionario\n",
    "bow.vocabulary_\n",
    "#Selecao de uma message específica\n",
    "m1 = df_descritivo['messages'][0]\n",
    "#Veja a BOW da message\n",
    "bow1 = bow.transform([m1])\n",
    "print(bow1)\n",
    "#Selecione o termo de maior frequencia\n",
    "#print(bow.get_feature_names()[35934])\n",
    "#Analise de termos relevantes\n",
    "termos = bow.get_feature_names()\n",
    "df_termos = pd.DataFrame(termos,columns=['termos'])\n",
    "df_termos[df_termos['termos'].str.contains('valor')]\n",
    "#Transformacao da BOW em um DataFrame esparso completo de mensagens\n",
    "df_bow = bow.transform(df_descritivo['messages'])\n",
    "#Algumas informações técnicas da BOW\n",
    "print('Shape of Sparse Matrix: ', df_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', df_bow.nnz)\n",
    "sparsity = (100.0 * df_bow.nnz / (df_bow.shape[0] * df_bow.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peso e Normalização com TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer().fit(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploração da TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 345)\t0.177741860267\n",
      "  (0, 330)\t0.218162826091\n",
      "  (0, 329)\t0.218162826091\n",
      "  (0, 302)\t0.201543762523\n",
      "  (0, 227)\t0.218162826091\n",
      "  (0, 224)\t0.218162826091\n",
      "  (0, 211)\t0.397401440144\n",
      "  (0, 103)\t0.345246630706\n",
      "  (0, 91)\t0.345246630706\n",
      "  (0, 82)\t0.345246630706\n",
      "  (0, 59)\t0.345246630706\n",
      "  (0, 12)\t0.320466877323\n",
      "624\n",
      "  (0, 345)\t0.177741860267\n",
      "  (0, 330)\t0.218162826091\n",
      "  (0, 329)\t0.218162826091\n",
      "  (0, 302)\t0.201543762523\n",
      "  (0, 227)\t0.218162826091\n",
      "  (0, 224)\t0.218162826091\n",
      "  (0, 211)\t0.397401440144\n",
      "  (0, 103)\t0.345246630706\n",
      "  (0, 91)\t0.345246630706\n",
      "  (0, 82)\t0.345246630706\n",
      "  (0, 59)\t0.345246630706\n",
      "  (0, 12)\t0.320466877323\n",
      "  (1, 644)\t0.216289054799\n",
      "  (1, 639)\t0.239633320848\n",
      "  (1, 594)\t0.239633320848\n",
      "  (1, 548)\t0.239633320848\n",
      "  (1, 509)\t0.216289054799\n",
      "  (1, 483)\t0.18025727109\n",
      "  (1, 440)\t0.216289054799\n",
      "  (1, 420)\t0.160363631464\n",
      "  (1, 390)\t0.166083473172\n",
      "  (1, 383)\t0.166083473172\n",
      "  (1, 370)\t0.1631341215\n",
      "  (1, 359)\t0.172623051475\n",
      "  (1, 345)\t0.132908812871\n",
      "  :\t:\n",
      "  (201, 378)\t0.221372862249\n",
      "  (201, 371)\t0.21561690772\n",
      "  (201, 276)\t0.250879948337\n",
      "  (201, 233)\t0.250879948337\n",
      "  (201, 184)\t0.469123369865\n",
      "  (201, 78)\t0.234561684932\n",
      "  (202, 567)\t0.43915210475\n",
      "  (202, 474)\t0.43915210475\n",
      "  (202, 371)\t0.188713006904\n",
      "  (202, 333)\t0.316815018402\n",
      "  (202, 332)\t0.250439097847\n",
      "  (202, 296)\t0.500878195694\n",
      "  (202, 233)\t0.219576052375\n",
      "  (202, 107)\t0.341312395759\n",
      "  (203, 584)\t0.591146713101\n",
      "  (203, 317)\t0.548717711741\n",
      "  (203, 264)\t0.591146713101\n",
      "  (205, 567)\t0.501759896673\n",
      "  (205, 474)\t0.501759896673\n",
      "  (205, 378)\t0.221372862249\n",
      "  (205, 371)\t0.21561690772\n",
      "  (205, 276)\t0.250879948337\n",
      "  (205, 233)\t0.250879948337\n",
      "  (205, 184)\t0.469123369865\n",
      "  (205, 78)\t0.234561684932\n"
     ]
    }
   ],
   "source": [
    "#Pesos dos termos em uma mensagem, usando TF-IDF \n",
    "tfidf1 = tfidf.transform(bow1)\n",
    "print(tfidf1)\n",
    "#Verificar apenas a IDF de termos relevantes\n",
    "tfidf.idf_[bow.vocabulary_['valor']]\n",
    "#Transforma as mensagens do TD_BOW em mensagens do DF_TFIDF\n",
    "df_tfidf = tfidf.transform(df_bow)\n",
    "#Analise da TF-IDF\n",
    "print(df_descritivo.size)\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
